{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using th'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"prideandprejeduce.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698418"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 176, 158, 916, 3, 321, 4, 1171, 30, 72, 2534, 41, 916, 23, 21]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125309"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7030\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  125306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,  176,  158,  916],\n",
       "       [ 176,  158,  916,    3],\n",
       "       [ 158,  916,    3,  321],\n",
       "       [ 916,    3,  321,    4],\n",
       "       [   3,  321,    4, 1171],\n",
       "       [ 321,    4, 1171,   30],\n",
       "       [   4, 1171,   30,   72],\n",
       "       [1171,   30,   72, 2534],\n",
       "       [  30,   72, 2534,   41],\n",
       "       [  72, 2534,   41,  916]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[   1  176  158]\n",
      " [ 176  158  916]\n",
      " [ 158  916    3]\n",
      " [ 916    3  321]\n",
      " [   3  321    4]\n",
      " [ 321    4 1171]\n",
      " [   4 1171   30]\n",
      " [1171   30   72]\n",
      " [  30   72 2534]\n",
      " [  72 2534   41]]\n",
      "Response:  [ 916    3  321    4 1171   30   72 2534   41  916]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             70300     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7030)              7037030   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,156,330\n",
      "Trainable params: 20,156,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 6.2720\n",
      "Epoch 1: loss improved from inf to 6.27204, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 303s 153ms/step - loss: 6.2720\n",
      "Epoch 2/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 5.6509\n",
      "Epoch 2: loss improved from 6.27204 to 5.65087, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 163ms/step - loss: 5.6509\n",
      "Epoch 3/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 5.2944\n",
      "Epoch 3: loss improved from 5.65087 to 5.29442, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 5.2944\n",
      "Epoch 4/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 5.0525\n",
      "Epoch 4: loss improved from 5.29442 to 5.05252, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 320s 164ms/step - loss: 5.0525\n",
      "Epoch 5/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 4.8450\n",
      "Epoch 5: loss improved from 5.05252 to 4.84501, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 163ms/step - loss: 4.8450\n",
      "Epoch 6/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 4.6425\n",
      "Epoch 6: loss improved from 4.84501 to 4.64247, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 323s 165ms/step - loss: 4.6425\n",
      "Epoch 7/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 4.4378\n",
      "Epoch 7: loss improved from 4.64247 to 4.43776, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 319s 163ms/step - loss: 4.4378\n",
      "Epoch 8/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 4.2304\n",
      "Epoch 8: loss improved from 4.43776 to 4.23043, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 322s 164ms/step - loss: 4.2304\n",
      "Epoch 9/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 4.0147\n",
      "Epoch 9: loss improved from 4.23043 to 4.01466, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 327s 167ms/step - loss: 4.0147\n",
      "Epoch 10/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 3.7939\n",
      "Epoch 10: loss improved from 4.01466 to 3.79389, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 3.7939\n",
      "Epoch 11/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 3.5660\n",
      "Epoch 11: loss improved from 3.79389 to 3.56601, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 320s 163ms/step - loss: 3.5660\n",
      "Epoch 12/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 3.3408\n",
      "Epoch 12: loss improved from 3.56601 to 3.34083, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 309s 158ms/step - loss: 3.3408\n",
      "Epoch 13/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 3.1115\n",
      "Epoch 13: loss improved from 3.34083 to 3.11155, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 308s 157ms/step - loss: 3.1115\n",
      "Epoch 14/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 2.8803\n",
      "Epoch 14: loss improved from 3.11155 to 2.88027, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 319s 163ms/step - loss: 2.8803\n",
      "Epoch 15/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 2.6439\n",
      "Epoch 15: loss improved from 2.88027 to 2.64389, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 158ms/step - loss: 2.6439\n",
      "Epoch 16/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 2.4030\n",
      "Epoch 16: loss improved from 2.64389 to 2.40302, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 159ms/step - loss: 2.4030\n",
      "Epoch 17/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 2.1682\n",
      "Epoch 17: loss improved from 2.40302 to 2.16823, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 160ms/step - loss: 2.1682\n",
      "Epoch 18/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.9434\n",
      "Epoch 18: loss improved from 2.16823 to 1.94337, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 158ms/step - loss: 1.9434\n",
      "Epoch 19/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.7301\n",
      "Epoch 19: loss improved from 1.94337 to 1.73008, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 159ms/step - loss: 1.7301\n",
      "Epoch 20/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.5475\n",
      "Epoch 20: loss improved from 1.73008 to 1.54755, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 304s 155ms/step - loss: 1.5475\n",
      "Epoch 21/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.3785\n",
      "Epoch 21: loss improved from 1.54755 to 1.37854, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 306s 157ms/step - loss: 1.3785\n",
      "Epoch 22/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.2350\n",
      "Epoch 22: loss improved from 1.37854 to 1.23502, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 307s 157ms/step - loss: 1.2350\n",
      "Epoch 23/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.1190\n",
      "Epoch 23: loss improved from 1.23502 to 1.11902, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 317s 162ms/step - loss: 1.1190\n",
      "Epoch 24/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 1.0248\n",
      "Epoch 24: loss improved from 1.11902 to 1.02476, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 159ms/step - loss: 1.0248\n",
      "Epoch 25/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.9456\n",
      "Epoch 25: loss improved from 1.02476 to 0.94561, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.9456\n",
      "Epoch 26/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.8857\n",
      "Epoch 26: loss improved from 0.94561 to 0.88571, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 307s 157ms/step - loss: 0.8857\n",
      "Epoch 27/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.8330\n",
      "Epoch 27: loss improved from 0.88571 to 0.83297, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 159ms/step - loss: 0.8330\n",
      "Epoch 28/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.7926\n",
      "Epoch 28: loss improved from 0.83297 to 0.79264, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 308s 157ms/step - loss: 0.7926\n",
      "Epoch 29/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.7574\n",
      "Epoch 29: loss improved from 0.79264 to 0.75739, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.7574\n",
      "Epoch 30/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.7288\n",
      "Epoch 30: loss improved from 0.75739 to 0.72883, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 158ms/step - loss: 0.7288\n",
      "Epoch 31/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.7038\n",
      "Epoch 31: loss improved from 0.72883 to 0.70379, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 158ms/step - loss: 0.7038\n",
      "Epoch 32/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6824\n",
      "Epoch 32: loss improved from 0.70379 to 0.68238, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 310s 158ms/step - loss: 0.6824\n",
      "Epoch 33/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6636\n",
      "Epoch 33: loss improved from 0.68238 to 0.66364, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.6636\n",
      "Epoch 34/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6474\n",
      "Epoch 34: loss improved from 0.66364 to 0.64736, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.6474\n",
      "Epoch 35/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6334\n",
      "Epoch 35: loss improved from 0.64736 to 0.63341, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 163ms/step - loss: 0.6334\n",
      "Epoch 36/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6185\n",
      "Epoch 36: loss improved from 0.63341 to 0.61854, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 160ms/step - loss: 0.6185\n",
      "Epoch 37/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.6065\n",
      "Epoch 37: loss improved from 0.61854 to 0.60646, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 163ms/step - loss: 0.6065\n",
      "Epoch 38/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5955\n",
      "Epoch 38: loss improved from 0.60646 to 0.59549, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 325s 166ms/step - loss: 0.5955\n",
      "Epoch 39/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5827\n",
      "Epoch 39: loss improved from 0.59549 to 0.58272, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.5827\n",
      "Epoch 40/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5767\n",
      "Epoch 40: loss improved from 0.58272 to 0.57669, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.5767\n",
      "Epoch 41/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5646\n",
      "Epoch 41: loss improved from 0.57669 to 0.56457, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 311s 159ms/step - loss: 0.5646\n",
      "Epoch 42/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5581\n",
      "Epoch 42: loss improved from 0.56457 to 0.55813, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 314s 160ms/step - loss: 0.5581\n",
      "Epoch 43/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5485\n",
      "Epoch 43: loss improved from 0.55813 to 0.54846, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 309s 158ms/step - loss: 0.5485\n",
      "Epoch 44/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5391\n",
      "Epoch 44: loss improved from 0.54846 to 0.53915, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 314s 160ms/step - loss: 0.5391\n",
      "Epoch 45/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5358\n",
      "Epoch 45: loss improved from 0.53915 to 0.53584, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 316s 162ms/step - loss: 0.5358\n",
      "Epoch 46/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5270\n",
      "Epoch 46: loss improved from 0.53584 to 0.52702, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.5270\n",
      "Epoch 47/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5219\n",
      "Epoch 47: loss improved from 0.52702 to 0.52190, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 317s 162ms/step - loss: 0.5219\n",
      "Epoch 48/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5140\n",
      "Epoch 48: loss improved from 0.52190 to 0.51403, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.5140\n",
      "Epoch 49/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5105\n",
      "Epoch 49: loss improved from 0.51403 to 0.51054, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 0.5105\n",
      "Epoch 50/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5048\n",
      "Epoch 50: loss improved from 0.51054 to 0.50479, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 332s 169ms/step - loss: 0.5048\n",
      "Epoch 51/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.5002\n",
      "Epoch 51: loss improved from 0.50479 to 0.50022, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 323s 165ms/step - loss: 0.5002\n",
      "Epoch 52/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4937\n",
      "Epoch 52: loss improved from 0.50022 to 0.49368, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.4937\n",
      "Epoch 53/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4892\n",
      "Epoch 53: loss improved from 0.49368 to 0.48921, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 312s 159ms/step - loss: 0.4892\n",
      "Epoch 54/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4845\n",
      "Epoch 54: loss improved from 0.48921 to 0.48455, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 319s 163ms/step - loss: 0.4845\n",
      "Epoch 55/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4786\n",
      "Epoch 55: loss improved from 0.48455 to 0.47864, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.4786\n",
      "Epoch 56/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4770\n",
      "Epoch 56: loss improved from 0.47864 to 0.47699, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 317s 162ms/step - loss: 0.4770\n",
      "Epoch 57/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4753\n",
      "Epoch 57: loss improved from 0.47699 to 0.47528, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.4753\n",
      "Epoch 58/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4688\n",
      "Epoch 58: loss improved from 0.47528 to 0.46879, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 321s 164ms/step - loss: 0.4688\n",
      "Epoch 59/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4659\n",
      "Epoch 59: loss improved from 0.46879 to 0.46591, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 0.4659\n",
      "Epoch 60/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4626\n",
      "Epoch 60: loss improved from 0.46591 to 0.46257, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.4626\n",
      "Epoch 61/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4592\n",
      "Epoch 61: loss improved from 0.46257 to 0.45921, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.4592\n",
      "Epoch 62/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4593\n",
      "Epoch 62: loss did not improve from 0.45921\n",
      "1958/1958 [==============================] - 320s 163ms/step - loss: 0.4593\n",
      "Epoch 63/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4533\n",
      "Epoch 63: loss improved from 0.45921 to 0.45334, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 325s 166ms/step - loss: 0.4533\n",
      "Epoch 64/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4510\n",
      "Epoch 64: loss improved from 0.45334 to 0.45099, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 320s 163ms/step - loss: 0.4510\n",
      "Epoch 65/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4472\n",
      "Epoch 65: loss improved from 0.45099 to 0.44719, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 163ms/step - loss: 0.4472\n",
      "Epoch 66/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4475\n",
      "Epoch 66: loss did not improve from 0.44719\n",
      "1958/1958 [==============================] - 315s 161ms/step - loss: 0.4475\n",
      "Epoch 67/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4435\n",
      "Epoch 67: loss improved from 0.44719 to 0.44354, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 314s 160ms/step - loss: 0.4435\n",
      "Epoch 68/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4421\n",
      "Epoch 68: loss improved from 0.44354 to 0.44215, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 0.4421\n",
      "Epoch 69/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4367\n",
      "Epoch 69: loss improved from 0.44215 to 0.43665, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 318s 162ms/step - loss: 0.4367\n",
      "Epoch 70/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4371\n",
      "Epoch 70: loss did not improve from 0.43665\n",
      "1958/1958 [==============================] - 319s 163ms/step - loss: 0.4371\n",
      "Epoch 71/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4336\n",
      "Epoch 71: loss improved from 0.43665 to 0.43361, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 316s 161ms/step - loss: 0.4336\n",
      "Epoch 72/72\n",
      "1958/1958 [==============================] - ETA: 0s - loss: 0.4300\n",
      "Epoch 72: loss improved from 0.43361 to 0.43002, saving model to next_words.h5\n",
      "1958/1958 [==============================] - 313s 160ms/step - loss: 0.4300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d81f32c5c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=72, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'ebook']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 296, in assert_input_compatibility\n",
      "        f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
      "\n",
      "['The', 'Project', 'Gutenberg']\n",
      "1/1 [==============================] - 1s 640ms/step\n",
      "tm\n",
      "['Gutenberg', 'eBook', 'of']\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "pride\n",
      "['abuse', 'your', 'own']\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "children\n",
      "['five', 'times', 'as']\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "pretty\n",
      "['']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\sena\\anaconda3\\envs\\mywordpredenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 296, in assert_input_compatibility\n",
      "        f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 0)\n",
      "\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('mywordpredenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82e5d6953606c0b32bfb7b629bc16e8f995826e8ac16bddeebb37e2ccc5ac928"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
